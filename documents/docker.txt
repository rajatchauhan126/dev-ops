"Docker docs" 
-----------------------------------------Introduction-------------------------------
Docker is a full development platform for creating containerized apps,


-----------------------------------------Installation-------------------------------

--Open official website of docker 
    --https://www.docker.com/get-started

--download for windows
    --Requires Microsoft Windows 10 Professional or Enterprise 64-bit. For previous versions get Docker Toolbox.

--------AWS Installation
--https://docs.aws.amazon.com/AmazonECS/latest/developerguide/docker-basics.html#install_docker

---------commands
--docker run -d busybox:1.24 sleep 1000
--docker ps
--docker run --rm busybox:1.24 sleep 1
--docker ps -a //to list all the container
--docker run --name hello_world busybox:1.24 //specify which image to run
--docker ps -a

-------low level of docker image
-- docker run -d busybox:1.24 sleep 100//detach mode
-- docker inspect "cf269eefe56ec0c3a171ffbc40355fdd5b1c83382efdbf63265283dc08bbbe7c"//the ip address and mac address etc

---------docker port mapping and docker logs commands-----
-- tomcat image that execute java servlet

-- go to tomcat page //offical site


---------------------------------------Build Docker images by docker commit command--------
Note : type sudo su whenever commands are not executing, and exit command to come out of container

step 1 : go to docker hub, search debian.
step 2 : there are list of images there, we will use jessie image.
step 3 : go to putty, -> type sudo su 
step 4 : docker run -it debain:jessie
step 5 : ls
stpe 6 : type git, will get error
step 7 : apt-get update && apt-get install git
step 8 : type git, now it will show help option
step 9 : docker ps -a//docker commit command would save the changes we made to the docker container's file system to a new image.
step 10 :docker commit dockerid dockerhub repository name:1.00 //return id of new image and tags
    //base image is debian and extended image is my username
step 11 : docker images
step 12 :  docker run -it Repository:1.00     //repository you just created
step 13 : ls //to see the folder strcture.


---------------------






----------------------------------------Basic commands------------------------------
--Installation steps link : https://www.digitalocean.com/community/tutorials/how-to-install-and-use-docker-on-ubuntu-18-04

-------------------------Build docker images by writing dockerfile------------
-- touch Dockerfile //without any extension
-- vi Dockerfile
-- 
    FROM debian:jessie //from base image, we will use debian jessie 
    RUN apt-get update //command to execute
    RUN apt -get install -y git
    RUN apt-get install -y vim

--save the file and exit 

-- docker build -t jameslee/debain . //docker build will build file using the docker file

-- docker images

----------------------------Best practice to write Docker file-----------------
-- will work with old Dockerfile
            FROM debian:jessie //from base image, we will use debian jessie 
    RUN apt-get update //command to execute
    RUN apt -get install -y git
    RUN apt-get install -y vim

--change to one line only
            FROM debian:jessie //from base image, we will use debian jessie 
    RUN apt-get update && apt -get install -y git && apt-get install -y vim

or

RUN apt-get update && apt -get install -y \ 
    git \ 
    vim

-- continue with the same file and append

CMD ["echo", "Hello world"]

--docker cache--
    if old image has not change, then it will take the copy of old image from cache
    and load from cache only. this will improve the performance

 1) chain instruction
 2) invalidate cache-- --no-cache=true
    docker build -t jameslee/debain . --no-cache=true


--copy container, copy a file to the constructor

FROM debain:jessie
RUN apt-get update && apt-get install -y \
    git \
    vim
COPY abc.txt /src/abc.txt

--save and exit
--rebuild the image, using docker  build -t jameslee/debain .
--docker run -it imagename
--check in src folder

Add instrunction
--all to download from internet and copy to container.
--ability to automatically unpack compressed files.
--copy is preferred over add.
--try to use copy.

------------------Push images to docker hub---------
--go to docker hub, login

--push a image
    docker_hub id / repository name  //id - username ,

-- docker images
-- docker tag imageid username/debain //imageid choose from image list

--latest tag -- default tag when no tag is provided, not work in reality just a convension
    avoid using latest tag.

-- docker images
-- docker login --username=jleetutorial //push to docker hub
-- docker push jleetutorial/debain:1:01 //1:01 = docker tag

----------containerize hello_world container----------
//using flask image for example

clone the repository and start from below build command
git clone -b v0.1 http://github.com/jleetutorial/dockerapp.git

------or

// create a folder, then create two files Dockerfile and app.py
// write below code in app

FROM flask import Flask 
app = Flask(__name__)

@app.route('/')
def hello_world():
    return 'Hello, world!'

if __name__ == '__main__':
    app.run(host='0.0.0.0')

//save and exit

//write the below code in Dockerfile

FROM python:3.5
RUN pip install Flask=0.11.1
RUN useradd -ms /bin/bash admin
USER admin
WORKDIR /app
COPY app /app
CMD ["python", "app.py"]

//save and exit

-- docker build -t dockerapp:v0.1 .
-- docker images
-- docker run -d -p 5000:5000 imageId //-d to run in background, -p for 

//access the python server, for linux

//or , use the IPv4 Public IP 3.16.111.111 from aws , and also make sure that security group inbound is set to all traffic.


-- docker-machine ls //give the ip
-- open browser ip:5000

-----------login into container-------
-- docker exec -it containerid bash  //login as admin user

-- ps axu 

-------------implement a simple key-value loopup service--------

1)

2) using redis //key value pair
    -in memory data strcture store, used as db, cache, message broker.
    -build in replication and different level of on-disk persistance.
    -widely used in product like facebook news feeds, twitter timeline service.

------go to browser and type //this will give us microservice approach

    https://pypi.org/project/redis/

-------integrate redis with python //

--update app.py to use redis

app.py

import redis


-----------------create docker links--------------------- //transfer b/w 2 container

-- docker urn -d --name redis redis:3.2.0
-- docker ps

-- docker build -t dockerapp:v0.3 .  //change the file first accordingly
-- docker run -d -p 5000:5000 --link redis dockerapp:v0.3
-- docker ps
-- docker exec -it redisconatinerid bash
-- more /etc/hosts
-- exit 

-- docker inspect redisconatinerid | grep IP 
-- login to app container and ping redis //docker inspect //no need for other port by using link

//recipient and source container

--------------------docker compose-----------------//for lot of container 
    --tool for defining and running multi-container docker applications.
    --we create Compose file to configure our applications services.
    -- then using single command, we create and start all the services from our configurations.

-- install docker compose if not install
-- docker-compose version // to check if install
-- touch docker-compose.yml //write docker-compose.yml, 3 version of docker format, we will use version 3.

vi docker-compose.yml
    version: '3'
    services: 
        dockerapp:      //how to build and run the container
            build: .
            ports:
                - "5000:5000"
            depends_on:
                - redis 
        redis:
            image: redis:3.2.0  //from existing or new images

// in version 3, we don't need to link the container.
-- //stop all container before proceed
-- docker ps
-- docker stop containerid

--docker-compose up -d //to start the build process, -d in detach
-- docker-compose ps
-- docker-compose logs // -f for , dockerapp to find the logs of that container
-- docker-compose rm --all // to remove
-- docker-compose ps

//all container will be schedule in one current node.
//in sworm it will run in multiple nodes.

-- docker ps //to verify if images are running

-- docker-compose build // force new image

-------------------Docker networking----------------------
-- default network - 
1. closed network/ none network
2. bridge network
3. host network
4. overlay network

-- docker network ls

-- 1. none network - no access to outside world, isolated container.

--  docker run -d --net none busybox sleep 1000 //run with none
-- docker exec -t containerid /bin/ash

//inside the container, from other container accessable.

-- ping 8.8.8.8 //ping not okay
-- ifconfig ,only one interface, only use with internal machine, no access to other. use for maximum security.

--2.  //container have access to two network interfaces, a loopback interface, a private interface. //for small network
-- docker network inspect bridge
-- docker run -d --name container_1 busybox sleep 1000
-- docker exec -it container_1 ifconfig

-- docker run -d --name container_2 busybox sleep 1000 //try 
-- docker exec -it container_2 ifconfig

-- ping from container_1 to container_2 ip. //ping 172.17.0.3
-- docker exec -it container_1 ping 8.8.8.8  //also, work, google public dns

//create another bridge
--docker network create --driver bridge my_bridge_network
--docker network ls 
--docker network inspect my_bridge_network//check the range

// fire container_3 from another new bridge network

-- docker run -d --name container_3 --net my_bridge_network busybox sleep 1000

// ping one from 3, get the inspect

-- docker exec -it container_3 ping 172.12.0.2  //can't reach
-- docker network connect bridge container_3
-- docker exec -it container_3 ifconfig

-- docker exec -it container_3 ping 172.12.0.2 //now reach
-- -- docker network disconnect bridge container_3
-- docker exec -it container_3 ping 172.12.0.2 //not reach

--3. //less protected, anyone can access, no isolation, give performance, 

-- docker run -d --name container_4 --net host busybox sleep 1000
-- docker exec -it container_4 ifconfig //all network interface are there.

--4. // Accross multiple host machine networking.
    //requisites - 
        1. when run in sworm mode,overlay on manager node.
        2. A key-value store such as consul .

    //mostly used in production.

-- docker-compose download

-- vi docker-compose.yml

    add in last

    version: '3'
    services: 
        dockerapp:      //how to build and run the container
            build: .
            ports:
                - "5000:5000"
            depends_on:
                - redis 
            networks:
                - my_net
        redis:
            image: redis:3.2.0  //from existing or new images
            networks:
                - my_net
    networks:
        my_net:
            driver: bridge

// save and exit

-- docker compose up -d

// this time network creted with driver bridge.

---------------write unit test in docker container-----------------

--create a file test.py in same where app.py is present

import unittest
import app

class TestDockerapp(unittest.TestCase):

    def setUp(self):
        self.app=app.app.test_client()

    def test_save_value(self):
        response = self.app.post('/', data=dict(submit='save', key='2', cache_value='two'))
        assert response.status_code == 200
        assert b'2' in response.data
        assert b'two' in response.data

    def test_load_value(self):
        self.app.post('/', data=dict(submit='save', key='2', cache_value='two'))
        response = self.app.post('/', data=dict(submit='load', key='2'))
        assert response.status_code == 200
        assert b'2' in response.data
        assert b'two' in response.data

if __name__'__main__':
    unittest.main()

//save and exit

-- dcokr-compose up -d
-- docker-compose run dockerapp python test.py 

-----------------Continuous intergration------------------------

--add ssh key to github

    https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/

-----------link circle3 CI with github accout-----

-- clone the repo using ssh 
    https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/
 
 //need to fork this repository then this repository will be moved to your github.

--  git clone git@github.com:rajatchauhan126/dockerapp.git

-- login to circleci using github account

-- add project -> select the github repository, go with default option
    start build -> //it will fails this time. as we have not set password

// now checkout to branch branch-v0.6
  --  git checkout branch branch-v0.6
  --  git checkout -b test-ci
  --  mkdir dummy.txt
  -- git add .
  -- git commit -m "add a dummmy file"
  -- git push origin test-ci

  //automatically build will be trigger in the circleci

  -------------publish docker image to docker hub from circleci---

  -- go to circleci -> hit the button on the circle top side. 
  -> select the success build -> top right, select project settings -> 
    envionment variables -> Add variables

    DOCKER_HUB_EMAIL     xxxx.com
    DOCKER_HUB_PWD       xxxx@999
    DOCKER_HUB_USER_ID   xxxxn234  
        
// go to fork docker repository 
    -- git stash && git checkout branch-v0.6
    -- git checkout -b circle_ci_publish

// add  below line in config.yml , original code is in the master branch
//imp link,  https://help.github.com/articles/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent/


      -deploy:
          name: Push application Dcoker image
          command: |
            docker login -e $DOCKER_HUB_EMAIL -u $DOCKER_HUB_USER_ID -p $DOCKER_HUB_PWD
            docker tag dockerapp_dockerapp $DOCKER_HUB_USER_ID/dockerapp:$CIRCLE_SHA1
            docker tag dockerapp_dockerapp $DOCKER_HUB_USER_ID/dockerapp:latest
            docker push $DOCKER_HUB_USER_ID/dockerapp:$CIRCLE_SHA1
            docker push $DOCKER_HUB_USER_ID/dockerapp:latest

//save and auto build be trigger in Circleci , goto circleci and you will find a success build

---------deploy docker app to cloud-------------

-- install the docker machine
-- docker-machine create --driver digitalocean --digitalocean-access-token tokenfrom digitalocean doker-app-machine// we can go for aws
-- docker-machine env docker-app-machine
-- set the env

-- copy the docker-compose.yml prod.yml
-- open prod.yml

    change, image: rajatchauhan126/dockerapp //new dockerhub account

-- docker-compose -f prod.yml up -d
-- docker-machine ls

/---------------

docker-machine create --driver amazonec2 --amazonec2-access-key AKIAJV552EBUR64PKMSA --amazonec2-secret-key  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 aws01
//docker machine for deploy in cloud